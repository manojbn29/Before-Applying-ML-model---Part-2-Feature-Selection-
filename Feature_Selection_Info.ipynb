{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arctic-melbourne",
   "metadata": {},
   "source": [
    "# **Feature selection Info**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-disability",
   "metadata": {},
   "source": [
    "* In feature selection we use sklearn.feature_selection module to get good accuracy score and to avoid curse od dimensionality.\n",
    "\n",
    "* Always we first do train test split and then we will apply the feature selection technique to avoid the overfitting.\n",
    "\n",
    "* To know deeper about this read this : https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-lightning",
   "metadata": {},
   "source": [
    "## Diffrent types of feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-python",
   "metadata": {},
   "source": [
    "\n",
    "1) Removing features with low variance (VarianceThreshold)\n",
    "2) Univariate feature selection:\n",
    "\n",
    "            a) For regression: f_regression, mutual_info_regression\n",
    "            b) For classification: chi2, f_classif, mutual_info_classif\n",
    "\n",
    "\n",
    "3) Correlation Matrix (Using Pearson correlation)\n",
    "4) Recursive feature elimination\n",
    "5) Feature selection using SelectFromModel:\n",
    "\n",
    "            a) L1-based feature selection:\n",
    "                    a.1) Regression : Lasso\n",
    "                    a.2) Classification : LogisticRegression and Linear SVM\n",
    "                    \n",
    "            b) Tree-based feature selection\n",
    "            \n",
    "6) Sequential Feature Selection\n",
    "7) Feature selection as part of a pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-spectacular",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hollow-moderator",
   "metadata": {},
   "source": [
    "## 1) Removing features with low variance (VarianceThreshold)\n",
    "\n",
    "* It removes all features whose variance doesnâ€™t meet some threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-reach",
   "metadata": {},
   "source": [
    "## 2) Univariate feature selection:\n",
    "\n",
    "* Univariate feature selection works by selecting the best features based on univariate statistical tests.\n",
    "\n",
    "* Here we extarct the top features manually or by using SelectKBest, SelectPercentile modules\n",
    "\n",
    "* For regression we use -->  f_regression, mutual_info_regression models\n",
    "\n",
    "* For classification we use --> chi2, f_classif, mutual_info_classif models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-england",
   "metadata": {},
   "source": [
    "## 3) Correlation Matrix (Using Pearson correlation)\n",
    "\n",
    "* Correlation states how the features are related to each other or the target variable.\n",
    "\n",
    "* Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n",
    "\n",
    "* Heatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-ontario",
   "metadata": {},
   "source": [
    "## 4) Recursive feature elimination\n",
    "\n",
    "* The goal of RFE is to select features by recursively considering smaller and smaller sets of features.\n",
    "\n",
    "* First, the estimator is trained on the initial set of features and the importance of each feature is obtained through coef_ or feature_importances_\n",
    "\n",
    "* Then, the least important features are pruned from current set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-makeup",
   "metadata": {},
   "source": [
    "## 5) Feature selection using SelectFromModel:\n",
    "\n",
    "* Generally it is applied inside the ml model \n",
    "\n",
    "* There are 2 types in this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-charge",
   "metadata": {},
   "source": [
    "### a) L1-based feature selection:\n",
    "\n",
    "* Linear models penalized with the L1 norm have sparse solution.\n",
    "\n",
    "* Here we can SelectFromModel library to select the non-zero coefficients.\n",
    "\n",
    "* For Regression we use Lasso model \n",
    "\n",
    "* For classification we use LogisticRegression and Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-language",
   "metadata": {},
   "source": [
    "### b) Tree-based feature selection\n",
    "\n",
    "* Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute impurity-based feature importances\n",
    "\n",
    "* In turn can be used to discard irrelevant features (when coupled with the SelectFromModel meta-transformer):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-louis",
   "metadata": {},
   "source": [
    "## 6) Sequential Feature Selection\n",
    "\n",
    "* Sequential Feature Selection [sfs] (SFS) is available in the SequentialFeatureSelector transformer. SFS can be either forward or backward:\n",
    "\n",
    "* Use this link to get more info on this: https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "* us this link to get more more info: https://scikitlearn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-featureselection-plot-select-from-model-diabetes-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-ceiling",
   "metadata": {},
   "source": [
    "## 7) Feature selection as part of a pipeline\n",
    "\n",
    "* Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a Pipeline."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fatal-advisory",
   "metadata": {},
   "source": [
    "eg code for this is : \n",
    "    \n",
    "clf = Pipeline([\n",
    "                 ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "                 ('classification', RandomForestClassifier())])\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "In this snippet we make use of a LinearSVC coupled with SelectFromModel to evaluate feature importances and select the most relevant features. Then, a RandomForestClassifier is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the Pipeline examples for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
